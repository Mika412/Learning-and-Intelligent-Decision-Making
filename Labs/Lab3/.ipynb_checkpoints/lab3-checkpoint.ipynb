{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 3: Partially observable Markov decision problems\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modeling\n",
    "\n",
    "Consider once again the POMDP problem from the homework and represented in the transition diagram below.\n",
    "\n",
    "<img src=\"pomdp.png\" width=\"400px\">\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* All transitions occur with probability 1 except those from state $A$, where the probabilities are indicated under the edge label.\n",
    "\n",
    "* At each step, the agent makes an observation corresponding to the letter in the state designation. Such observation occurs with probability 1.\n",
    "\n",
    "Consider throughout that $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your POMDP in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* Create a list with all the observations\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* For each action, define a `numpy` array with the corresponding observation probabilities;\n",
    "* Define a `numpy` array with the cost describing the problem.\n",
    "\n",
    "The order for the states and actions used in the transition probability and cost matrices should match that in the lists of states and actions. \n",
    "\n",
    "**Note**: Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = ['A', 'B1', 'B2', 'C', 'D', 'E', 'F']\n",
    "\n",
    "actions = ['a', 'b', 'c']\n",
    "\n",
    "observations = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "pA = np.array([[0,0.5,0.5,0,0,0,0],\n",
    "               [0,0,0,0,0,1,0],\n",
    "               [0,0,0,0,0,0,1],\n",
    "               [0,1,0,0,0,0,0],\n",
    "               [0,0,1,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0]])\n",
    "\n",
    "pB = np.array([[0,0.5,0.5,0,0,0,0],\n",
    "               [0,0,0,0,0,0,1],\n",
    "               [0,0,0,0,0,1,0],\n",
    "               [0,1,0,0,0,0,0],\n",
    "               [0,0,1,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0]])\n",
    "            \n",
    "pC = np.array([[0,0.5,0.5,0,0,0,0],\n",
    "               [0,0,0,1,0,0,0],\n",
    "               [0,0,0,0,1,0,0],\n",
    "               [0,1,0,0,0,0,0],\n",
    "               [0,0,1,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0]])\n",
    "\n",
    "obs = np.array([[1,0,0,0,0,0],\n",
    "               [0,1,0,0,0,0],\n",
    "               [0,1,0,0,0,0],\n",
    "               [0,0,1,0,0,0],\n",
    "               [0,0,0,1,0,0],\n",
    "               [0,0,0,0,1,0],\n",
    "               [0,0,0,0,0,1]])\n",
    "\n",
    "cost = np.array([[1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [1, 1, 1],\n",
    "                 [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sampling\n",
    "\n",
    "You are now going to sample random trajectories of your POMDP and observe the impact it has on the corresponding belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Generate a random POMDP trajectory using a uniformly random policy. In particular, from a random initial state $x_0$ generate:\n",
    "\n",
    "1. A sequence of 10,000 states by selecting the actions uniformly at random;\n",
    "2. The corresponding sequence of 10,000 actions;\n",
    "3. The corresponding sequence of 10,000 observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "length = 10000\n",
    "\n",
    "#Uniformly random policy\n",
    "policy = np.array([[1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3],\n",
    "                   [1/3, 1/3, 1/3]])\n",
    "\n",
    "#Choose a random initial state xO\n",
    "x0 = states[randint(0, len(states)-1)]\n",
    "\n",
    "\n",
    "trajectory_seq = []\n",
    "action_seq = []\n",
    "observation_seq = []\n",
    "\n",
    "state = x0\n",
    "for i in range(length):\n",
    "    state_index = states.index(state)\n",
    "    action = np.random.choice(actions, p=policy[state_index])\n",
    "    observation = np.random.choice(observations, p=obs[state_index])\n",
    "    \n",
    "    trajectory_seq +=[state]\n",
    "    action_seq += [action]\n",
    "    observation_seq += [observation]\n",
    "   \n",
    "    newState = None\n",
    "    if action == 'a':\n",
    "        newState = np.random.choice(states, p=pA[state_index])\n",
    "    elif action == 'b':\n",
    "         newState = np.random.choice(states, p=pB[state_index])\n",
    "    elif action == 'c':\n",
    "        newState = np.random.choice(states, p=pC[state_index])\n",
    "    else:\n",
    "        raise ValueError('Invalid action.')\n",
    "    state = newState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "For the POMDP trajectory generated in Activity 2, compute the corresponding sequence of beliefs, assuming that the agent does not know its initial state. Report the resulting beliefs, ignoring duplicate beliefs or beliefs whose distance is smaller than $10^{-3}$.\n",
    "\n",
    "**Note 1:** You may want to define a function `belief_update` that receives a belief, an action and an observation and returns the updated belief.\n",
    "\n",
    "**Note 2:** To compute the distance between vectors, you may find useful `numpy`'s function `linalg.norm`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.14285714\n",
      " 0.        ]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.  0.5 0.5 0.  0.  0.  0. ]\n",
      "[0.  0.  0.  0.  0.  0.5 0. ]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n",
      "[0.  0.5 0.5 0.  0.  0.  0. ]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Devision by 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e4b460fc3d1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mbelief\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mnext_belief\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbelief_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbelief\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnewBelief\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_belief\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbelief_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mbelief_seq\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_belief\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-e4b460fc3d1e>\u001b[0m in \u001b[0;36mbelief_update\u001b[1;34m(belief, action, observation)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Devision by 0.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumerator\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Devision by 0."
     ]
    }
   ],
   "source": [
    "#Since tha agent does not know its initial state, his belief is distributed across all states\n",
    "b0 = np.array([1/7,1/7,1/7,1/7,1/7,1/7,1/7])\n",
    "\n",
    "\n",
    "def belief_update(belief, action, observation):\n",
    "    actionProb = None\n",
    "    if action == 'a':\n",
    "        actionProb = pA\n",
    "    elif action == 'b':\n",
    "        actionProb = pB\n",
    "    elif action == 'c':\n",
    "        actionProb = pC\n",
    "    else:\n",
    "        raise ValueError('Invalid action.')\n",
    "        \n",
    "        \n",
    "    sum = 0\n",
    "    t1 = np.dot(belief, actionProb)\n",
    "    t2 = np.diag(obs[:,observations.index(observation)])\n",
    "    numerator= np.dot(t1, t2)\n",
    "    print(numerator)\n",
    "    for factor in np.nditer(numerator):\n",
    "        sum += factor\n",
    "    if sum == 0:\n",
    "        raise ValueError('Division by 0.')\n",
    "    return numerator/sum\n",
    "\n",
    "def newBelief(belief, prev_beliefs):\n",
    "    for b in prev_beliefs:\n",
    "        if np.linalg.norm(belief - b) < 10e-3:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "belief_seq = [b0]\n",
    "\n",
    "belief = b0\n",
    "for i in range(length):\n",
    "    next_belief = belief_update(belief, action_seq[i], observation_seq[i])\n",
    "    if newBelief(next_belief, belief_seq):\n",
    "        belief_seq += [next_belief]\n",
    "        \n",
    "    belief = next_belief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solution methods\n",
    "\n",
    "In this section you are going to compare different non-exact solution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Compute the solution for the underlying MDP and report the corresponding optimal policy and optimal cost-to-go. \n",
    "\n",
    "** Note:** You may reuse code from previous labs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "For each of the beliefs computed in Activity 3, compute the action prescribed by:\n",
    "\n",
    "* The MLS heuristic;\n",
    "* The AV heuristic;\n",
    "* The Q-MDP heuristic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Suppose that the optimal cost-to-go function for the POMDP can be represented using the $\\alpha$-vectors\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\\right\\}$$\n",
    "\n",
    "where the first 3 vectors correspond to action $a$, the middle three vectors correspond to action $b$ and the last 3 vectors correspond to action $c$. Using the $\\alpha$-vectors above, \n",
    "\n",
    "* Represent the the optimal cost-to-go function for all beliefs of the form $\\mathbf{b}=[0, \\epsilon, 1-\\epsilon, 0, 0, 0, 0]$, with $\\epsilon\\in[0,1]$. \n",
    "* Compare the optimal policy with the MDP heuristics from Activity 5 in the beliefs computed in Activity 3.\n",
    "\n",
    "** Note: ** Don't forget to import `matplotlib`, and use the magic `%matplotlib notebook`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your comments here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
